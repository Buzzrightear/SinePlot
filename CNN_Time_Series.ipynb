{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Time_Series.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOaS8pLmHMpAMmo6AY0+Oe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Buzzrightear/SinePlot/blob/master/CNN_Time_Series.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVqCCJKVvAyn"
      },
      "source": [
        "CNN Time Series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJI2Oyobu8pq",
        "outputId": "bff92624-02ae-46f6-f52c-ce5410f19b1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "'''\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "'''\n",
        "\n",
        "#Partly taken from https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/ \n",
        "#Sine wave prediction using CNN\n",
        "import numpy as np\n",
        "#!pip install PyGnuplot\n",
        "#import PyGnuplot as pg\n",
        "import math\n",
        "import random\n",
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "from numpy import array\n",
        " \n",
        "################### split a univariate sequence into samples for multistep prediction########################\n",
        "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
        "\tX, y = list(), list()\n",
        "\tfor i in range(len(sequence)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps_in\n",
        "\t\tout_end_ix = end_ix + n_steps_out\n",
        "\t\t# check if we are beyond the sequence\n",
        "\t\tif out_end_ix > len(sequence):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty.append(seq_y)\n",
        "\treturn array(X), array(y) \n",
        "    \n",
        "##################Make testSet###########################\n",
        "def make_testSet(n_steps_in, n_steps_out ):\n",
        "    raw_seq_SeedValue = random.random() \n",
        "    testSet = [] \n",
        "    #Create sequence for testing of seen and unseen data\n",
        "    for i in range(n_steps_in + n_steps_out):\n",
        "        testSet.append(math.sin(raw_seq_SeedValue))\n",
        "        raw_seq_SeedValue += 0.1\n",
        "    #Split it into test training set and prediction set\n",
        "    unSeenData = testSet[n_steps_in:len(testSet)]\n",
        "    unSeenData3 = unSeenData\n",
        "    testSet = testSet[0:n_steps_in]\n",
        "    #Exaggerate diff between unseen data and predicted data to show up better on graph\n",
        "    unSeenData2 = []\n",
        "    for i in unSeenData:\n",
        "        unSeenData2.append(i*2)\n",
        "    unSeenData = unSeenData2\n",
        "    return testSet, unSeenData, unSeenData3\n",
        " \n",
        "##################Make prediction########################\n",
        "def make_prediction(testSet, n_steps_in, n_steps_out):\n",
        "    x_input = array(testSet)\n",
        "    x_input = x_input.reshape((1, n_steps_in, n_features)) #Suggests a single sample of n_steps?\n",
        "    yhat = model.predict(x_input, verbose=0)\n",
        "    #print (\"x_input is \", x_input)\n",
        "    #print(\"yhat prediction is \" , yhat)\n",
        "    return x_input, yhat\n",
        "\n",
        "\n",
        "#################Parse results for graph##################\n",
        "def parse_results(x_input, yhat, unSeenData):\n",
        "    x_input_list = []     #Parse x_input into 1D list - x_input_list\n",
        "    prediction_list = []  \n",
        "    prediction_list2 = []  \n",
        "    unSeenData_List = []\n",
        "    for i in x_input:\n",
        "        for j in i:\n",
        "            for k in j:\n",
        "                x_input_list.append(k)\n",
        "                prediction_list.append(None) #So that values appear to continue on from x_input sequence, populate list with None for length of x_input\n",
        "                unSeenData_List.append(None)\n",
        "    #Parse prediction sequence into 1D list - prediction_list\n",
        "    for i in yhat:\n",
        "        for j in i:\n",
        "            prediction_list.append(j) #Then append values from prediction sequence yhat on it\n",
        "            prediction_list2.append(j) #Append values from prediction sequence yhat to a list that will be used for calculating error\n",
        "            x_input_list.append(None)\n",
        "    #Parse \n",
        "    for i in unSeenData:\n",
        "        unSeenData_List.append(i)\n",
        "    return x_input_list, prediction_list, prediction_list2, unSeenData_List\n",
        "    \n",
        "    \n",
        "#################Calculate error & PA#########################\n",
        "def calculate_error(prediction_list2, unSeenData3):\n",
        "    rmse = np.sqrt(np.mean((array(prediction_list2) - array(unSeenData3))**2)) #Subtract the correct array of results from the predicted results array, \n",
        "    #square them so they're all positive, find the avg. of differences and then find the sqaure root of that - gives us the RMSE\n",
        "    #As a %:\n",
        "    PA =  100-((rmse/(np.sqrt(np.mean((array(unSeenData3))**2)))) * 100)#\n",
        "    return rmse, PA\n",
        "\n",
        "\n",
        "#############Put in graph:########################\n",
        "'''def make_graph(x_input_list, prediction_list, unSeenData_List):\n",
        "    pg.s([x_input_list, prediction_list, unSeenData_List], filename='CNN_output.out')  # save data into a file\n",
        "    pg.c('set title \"CNN Output\"; set xlabel \"x-axis\"; set ylabel \"y-axis\"')\n",
        "    #pg.c('set key center top')\n",
        "    pg.c(\"plot 'CNN_output.out' u 1 w l t 'x\\_input = Test Set'\")  # plot test set seen data\n",
        "    pg.c(\"replot  'CNN_output.out' u 2 w l  t 'yhat = Predicted value'\") # plot test set CNN predicted data\n",
        "    pg.c(\"replot  'CNN_output.out' u 3 w l t 'unSeenData = Correct value'\") # plot test set unseen actual data\n",
        "'''\n",
        "\n",
        "##############test_model#################################\n",
        "def test_model(run_number):\n",
        "    global testSet, unSeenData, unSeenData3\n",
        "    global x_input_list, prediction_list, prediction_list2, unSeenData_List\n",
        "    global rmse, PA\n",
        "    testSet, unSeenData, unSeenData3 = make_testSet(n_steps_in, n_steps_out)\n",
        "    x_input, yhat = make_prediction(testSet, n_steps_in, n_steps_out)\n",
        "    x_input_list, prediction_list, prediction_list2, unSeenData_List = parse_results(x_input, yhat, unSeenData)\n",
        "    rmse, PA = calculate_error(prediction_list2, unSeenData3)\n",
        "    print(f\"\\nRun {run_number}. RMSE: {rmse} and PA: {PA}\")\n",
        "    return PA\n",
        "    \n",
        " \n",
        "#Sort input sequence:\n",
        "raw_seq_SeedValue = 0.083\n",
        "\n",
        "# define input sequence\n",
        "raw_seq = []\n",
        "testSet = []\n",
        "x_input = []\n",
        "yhat = []\n",
        "\n",
        "# define list containers for storing output sequences\n",
        "unSeenData = []\n",
        "unSeenData2 = []\n",
        "unSeenData3 = []\n",
        "x_input_list = []\n",
        "prediction_list = []  \n",
        "prediction_list2 = []  \n",
        "unSeenData_List = []\n",
        "rmse = 0.0\n",
        "PA = 0.0\n",
        "PA_list = []\n",
        "counter = 5\n",
        "\n",
        "# choose a number of time steps for each sample\n",
        "n_steps_in = 3\n",
        "n_steps_out = 10000\n",
        "epochs2 = 2000\n",
        "\n",
        "# Create array of values for sine wave for training\n",
        "for i in range(11000):\n",
        "    raw_seq.append(math.sin(raw_seq_SeedValue))\n",
        "    raw_seq_SeedValue += 0.1\n",
        "\n",
        "# split into samples\n",
        "X, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n",
        "\n",
        "#Number of features - 1 because with univariate sequence, we just have the one variable (I think this relates to efectively having a single value input)\n",
        "n_features = 1\n",
        "\n",
        "# reshape from [samples, timesteps] into [samples, timesteps, features] - we have multiple samples, so input data needs to include number of variables/features being supplied/expected\n",
        "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
        "\n",
        "\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(n_steps_out))\n",
        "model.compile(optimizer='adam', loss='mse') \n",
        "#'The model is fit using the efficient Adam version of stochastic gradient descent and optimized using the mean squared error, or mse, loss function.'\n",
        "#https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ \n",
        "# We have created the structure of the model so now we have to fit the model to our training dataset\n",
        "model.fit(X, y, epochs=epochs2, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "#Run it 5 times to compare errors:\n",
        "for i in range(counter):\n",
        "    PA_list.append(test_model(i))\n",
        "print(\"Average PA from \" + str(counter) + \" runs  and input of \" + str(n_steps_in) + \" and output of \" + str(n_steps_out) + \" with \" + str(epochs2) + \" epochs is \" + str(np.mean(array(PA_list))))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-595921c768cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow_version 2.x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:TPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.test' has no attribute 'tpu_device_name'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMpIzNYDvAEY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJnHJbMsu9oK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}